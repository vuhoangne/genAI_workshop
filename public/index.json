[
{
	"uri": "http://localhost:1313/genai_workshop/",
	"title": "Building with Generative AI on AWS using PartyRock, Amazon Bedrock, and Amazon Q",
	"tags": [],
	"description": "",
	"content": "Overview: Workshop - Building with Generative AI on AWS using PartyRock, Amazon Bedrock, and Amazon Q Introduction Welcome to the \u0026ldquo;Building with Generative AI on AWS\u0026rdquo; workshop – where you\u0026rsquo;ll learn how to build powerful, intelligent applications using cutting-edge AWS services: PartyRock, Amazon Bedrock, and Amazon Q.\nIn the age of digital transformation, Generative AI opens up incredible opportunities to create smart, creative, and highly personalized applications. With AWS, you can quickly prototype and deploy AI-powered solutions without needing deep machine learning expertise.\nWorkshop Objectives Understand the fundamentals of Generative AI and Foundation Models (FMs). Explore how to build AI apps easily using PartyRock, a no-code/low-code platform. Gain hands-on experience with Amazon Bedrock, accessing leading foundation models via a unified API. Leverage Amazon Q to boost productivity, generate code, analyze data, and support business decision-making with AI. Key Topics Introduction to Generative AI and Foundation Models Building your first AI app with PartyRock Accessing and integrating models via Amazon Bedrock Enhancing your workflow with Amazon Q Hands-on project: building a real-world GenAI application using AWS tools ‍ Who Should Attend This workshop is ideal for:\nStudents and developers interested in modern AI technologies Businesses looking to integrate Generative AI into their products and services Cloud engineers and architects wanting to implement AI-powered solutions on AWS What You\u0026rsquo;ll Gain By the end of the workshop, you will:\nBe confident in building and deploying GenAI applications on AWS Know how to choose the right foundation models for your use cases Effectively use tools like PartyRock and Amazon Q to accelerate development and innovation Get ready to innovate with Generative AI on AWS!\nAgenda Introduction module 1 module 2 module 3 Wrapping up Clean up "
},
{
	"uri": "http://localhost:1313/genai_workshop/4-module3/4.3-building-agents-for-amazon-bedrock/4.3.1-debugging-lambda-functions-with-amazon-q/",
	"title": "Debugging Lambda Functions with Amazon Q",
	"tags": [],
	"description": "",
	"content": "AWS Lambda is a serverless compute service that lets you run applications and services without provisioning or managing servers. It automatically handles the underlying compute resources, allowing you to focus on your code and easily scale.\nIn this section, you will intentionally update the data_process_action Lambda function with several bugs. Your goal is to use Amazon Q to debug and fix the issues.\nGetting Started Open the data_process_action Lambda function in the Lambda Console. Click the Test button to invoke the function. In the test event configuration popup: Enter an event name like test-event. Use the following test event JSON to simulate an agent calling the function: { \u0026#34;agent\u0026#34;: { \u0026#34;alias\u0026#34;: \u0026#34;TSTALIASID\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;Agent-AWS\u0026#34;, \u0026#34;version\u0026#34;: \u0026#34;DRAFT\u0026#34;, \u0026#34;id\u0026#34;: \u0026#34;ADI6ICMMZZ\u0026#34; }, \u0026#34;sessionId\u0026#34;: \u0026#34;975786472213626\u0026#34;, \u0026#34;httpMethod\u0026#34;: \u0026#34;GET\u0026#34;, \u0026#34;sessionAttributes\u0026#34;: {}, \u0026#34;inputText\u0026#34;: \u0026#34;Can you get the number of records in the databse\u0026#34;, \u0026#34;promptSessionAttributes\u0026#34;: {}, \u0026#34;apiPath\u0026#34;: \u0026#34;/get_num_records\u0026#34;, \u0026#34;messageVersion\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;actionGroup\u0026#34;: \u0026#34;agent_action_group\u0026#34; } Click Test again to invoke the function. ‍ First-Time Error You\u0026rsquo;ll likely encounter an error because some dependencies are missing. Don’t worry, Amazon Q in the console will help you:\nClick the Q icon in the right navigation bar to chat with Amazon Q. Ask: How can I add the official prebuilt AWS pandas lambda layer to my lambda function without using the CLI? Follow Amazon Q’s instructions to integrate the Pandas Lambda Layer. Troubleshooting with Amazon Q While the Amazon Q chat is helpful for general AWS questions, for more specific guidance, use the Troubleshoot with Amazon Q feature.\nIn the Lambda Console, under the Test tab, run the test again by clicking Test. When the new error appears, click the Troubleshoot with Amazon Q button. Click Help me resolve to prompt Q for a solution. Follow Q’s suggestions to: Update the S3_OBJECT environment variable if needed. The file should be: clickstream_data.csv Fix other potential code issues with Q’s assistance. After Fixing Issues Retest the Lambda function to confirm the errors are resolved. Continue using Troubleshoot with Q for each subsequent error until your Lambda runs without issues. A successful run will return accurate data processing results and no errors. Need help? Here\u0026rsquo;s what to do next\nUpdate the S3_OBJECT Environment variable with clickstream_data.csv Fix the typo in the file path (replace /tmp/data,csv with /tmp/data.csv) in the Lambda function code. Change length to len in the Lambda function code Here\u0026rsquo;s the full code for reference\nimport os import json import pandas import boto3 S3_BUCKET = os.environ[\u0026#34;S3_BUCKET\u0026#34;] S3_OBJECT = os.environ[\u0026#34;S3_OBJECT\u0026#34;] def lambda_handler(event, context): # Print the received event to the logs print(\u0026#34;Received event: \u0026#34;) print(event) # Initialize response code to None response_code = None # Extract the action group, api path, and parameters from the prediction action = event[\u0026#34;actionGroup\u0026#34;] api_path = event[\u0026#34;apiPath\u0026#34;] inputText = event[\u0026#34;inputText\u0026#34;] httpMethod = event[\u0026#34;httpMethod\u0026#34;] print(f\u0026#34;inputText: {inputText}\u0026#34;) # Check the api path to determine which tool function to call if api_path == \u0026#34;/get_num_records\u0026#34;: s3 = boto3.client(\u0026#34;s3\u0026#34;) s3.download_file(S3_BUCKET, S3_OBJECT, \u0026#34;/tmp/data.csv\u0026#34;) df = pandas.read_csv(\u0026#34;/tmp/data.csv\u0026#34;) # Get count of dataframe count = len(df) response_body = {\u0026#34;application/json\u0026#34;: {\u0026#34;body\u0026#34;: str(count)}} response_code = 200 else: # If the api path is not recognized, return an error message body = {\u0026#34;{}::{} is not a valid api, try another one.\u0026#34;.format(action, api_path)} response_code = 400 response_body = {\u0026#34;application/json\u0026#34;: {\u0026#34;body\u0026#34;: str(body)}} # Print the response body to the logs print(f\u0026#34;Response body: {response_body}\u0026#34;) # Create a dictionary containing the response details action_response = { \u0026#34;actionGroup\u0026#34;: action, \u0026#34;apiPath\u0026#34;: api_path, \u0026#34;httpMethod\u0026#34;: httpMethod, \u0026#34;httpStatusCode\u0026#34;: response_code, \u0026#34;responseBody\u0026#34;: response_body, } # Return the list of responses as a dictionary api_response = {\u0026#34;messageVersion\u0026#34;: \u0026#34;1.0\u0026#34;, \u0026#34;response\u0026#34;: action_response} return api_response Testing the Agent Once the Lambda function is fixed, go back to the Agent and test it again by asking:\nCan you help with the data processing task of getting the number of records in the production database? This time, the Agent will be able to provide the correct answer. You can also view the trace to see how the Agent \u0026ldquo;thought\u0026rdquo; through the process to generate the correct response.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/1-introduction/",
	"title": "Introduction",
	"tags": [],
	"description": "",
	"content": "In this workshop you will complete the following 3 modules:\n️Build Generative AI Applications with PartyRock\nLearn how to quickly build Generative AI applications with no code.\nUse Foundation Models in Amazon Bedrock\nLearn how to use various foundation models to generate text and images using Amazon Bedrock.\nChat with your Documents\nLearn how to use Amazon Bedrock to \u0026ldquo;Chat with your documents\u0026rdquo;. We will explore how to build RAG applications highlighting Knowledge Bases, embeddings, and agents.\nTarget Audience This is a 300-level workshop targeted for software developers, so familiarity with using services such as AWS Lambda and writing Python code is helpful.\nNo AI/ML experience is necessary.\nTime This workshop is intended to be finished within 2 hours\nCost Note: This workshop was made to run at AWS events. You can follow along with your own account but it will incur costs (estimated around $10).\nGetting Started If you are using Workshop Studio, please make sure to access your provided AWS account by clicking Open AWS console in the bottom left menu.\nPlease take the workshop survey when you are finished.\n️ Disclaimers This workshop only works in us-east-1\nClean up instructions are listed here\nAll code for the workshop is located here\nAll code is covered under the MIT-0 license\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/4-module3/4.1-retrieval-augmented-generation-with-amazon-bedrock/",
	"title": "Retrieval Augmented Generation with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Before diving into the RAG workflow, it\u0026rsquo;s important to understand embeddings. Embeddings represent documents as vectors in a multi-dimensional space. These vectors capture the essence of the content in a format that machines can process. By converting text into embeddings, we enable computers to \u0026ldquo;understand\u0026rdquo; and compare different text passages based on their contextual similarity.\nEmbedding Visualization Once embeddings are generated, the next step is to store and organize them for efficient retrieval. This is where vector databases come into play. A vector database allows us to store and query embeddings, facilitating fast and relevant retrieval of documents based on their vector representation. Essentially, it serves as a bridge between raw data and actionable insights derived from large language models (LLMs). For this module, we’ll use FAISS.\n️ Powering RAG with Amazon Bedrock With the foundation of embeddings and vector databases in place, we are ready to implement the RAG workflow using Amazon Bedrock. This section demonstrates how to use large language models to process input queries, retrieve relevant document embeddings from a vector database, and generate coherent, contextually appropriate responses using LLMs. We’ll also leverage LangChain, a framework designed to simplify building LLM-powered applications.\nExercise 1: Getting Started with RAG Open rag_examples/base_rag.py. Let’s walk through the code to see how RAG works.\nWe start with a list of example sentences (line 15):\nsentences = [ # Pets \u0026#34;Your dog is so cute.\u0026#34;, \u0026#34;How cute your dog is!\u0026#34;, \u0026#34;You have such a cute dog!\u0026#34;, # Cities in the US \u0026#34;New York City is the place where I work.\u0026#34;, \u0026#34;I work in New York City.\u0026#34;, # Color \u0026#34;What color do you like the most?\u0026#34;, \u0026#34;What is your favorite color?\u0026#34;, ] Now let\u0026rsquo;s look at the implementation of the RAG workflow in rag_with_bedrock (line 60)\nCreate Bedrock Embeddings We initialize the embedding function by calling BedrockEmbeddings. We\u0026rsquo;re using the Amazon Titan text embedding model to convert text into a vector format for similarity comparison. embeddings = BedrockEmbeddings( client=bedrock_runtime, model_id=\u0026#34;amazon.titan-embed-text-v1\u0026#34;, ) Vector Search with FAISS We create a local vector store using FAISS.from_texts, which indexes the sentence embeddings into a searchable vector database. We can then vectorize a query and retrieve similar documents. local_vector_store = FAISS.from_texts(sentences, embeddings) docs = local_vector_store.similarity_search(query) Calling the RAG Prompt: We compile the content of the retrieved documents to form a context string. We then create a prompt that includes the context and the query. Finally, we call the call_claude function with our prompt get our answer. for doc in docs: context += doc.page_content prompt = f\u0026#34;\u0026#34;\u0026#34;Use the following pieces of context to answer the question at the end. {context} Question: {query} Answer:\u0026#34;\u0026#34;\u0026#34; model_id = \u0026#34;us.amazon.nova-lite-v1:0\u0026#34; system_prompts = [{\u0026#34;text\u0026#34;: \u0026#34;You are a helpful AI\u0026#34;}] message_1 = { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: [{\u0026#34;text\u0026#34;: f\u0026#34;{prompt}\u0026#34;}], } messages = [message_1] result = generate_conversation(model_id, system_prompts, messages) return result To test the code, run:\npython3 rag_examples/base_rag.py Try changing the query at line 109. For example:\ntry What city do I work in? Exercise 2: Chat with a PDF There is also an example of how you chat with a PDF. Inside rag_examples/chat_with_pdf.py we have the chunk_doc_to_text function that will ingest the PDF and chunk every 1000 characters to store in the vector database. This process can take a while depending on the server, so we have already chunked the data which is stored in the folder local_index.\nIn this example we stored all the text from the AWS Well Architected Framework which highlights best practices for designing and operating reliable, secure, efficient, cost-effective, and sustainable systems in the cloud.\nNow try running the code by entering the following command in the Terminal and pressing enter:\npython3 rag_examples/chat_with_pdf.py You can change the query on line 123. Play around to see how the model is able to use the context to figure out the correct answer. For example try:\nWhat are some good use cases for non-SQL databases? What is the Well-Architected Framework? How can I improve security in the cloud? You can even try off-topic questions such as what are popular ice cream flavors to see how the model handles questions outside the context.\nWrap up Now that you have gotten a taste of using Amazon Bedrock for RAG, let\u0026rsquo;s explore how we can create scalable RAG workflows.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/3-module2/3.1-use-amazon-bedrock-playground/",
	"title": "Use Amazon Bedrock Playground",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock Playground provides a quick way to experiment with different foundation models inside the AWS Console. You can compare model outputs, load example prompts, and even export API requests. The following modes are supported:\nChat/Text: Experiment with a variety of language processing tasks in a step-by-step interface.\n️ Image/Video: Easily generate compelling images/videos by providing text prompts to pre-trained models.\nYou can access the playground from the links above or from the Amazon Bedrock Console via the Playgrounds menu on the left. Take a few minutes to try out some examples.\nPlayground Examples Here are some examples you can try in each playground:\nChat/Text In Amazon Bedrock, choose: Select Chat / Text under Playgrounds. Click Select model. Choose Amazon Nova Pro. Click Apply. Try the following system prompt: You are an expert AWS Solutions Architect that helps customers create scalable, cost-efficient solutions on AWS How can I create a 3-tier web app on AWS? The sidebar contains model configuration options you can customize. Try changing the temperature to 1 to make the model more creative in its responses. ️ Image In Amazon Bedrock, choose: Select Image / Canvas under Playgrounds. Click Select model. Choose Amazon Nova Canvas. Click Apply. Try the following prompts: unicorns in a magical forest. Lots of trees and animals around. The mood is bright, and there is lots of natural lighting Downtown City, with lots of skyscrapers. At night time, lots of lights in the buildings. Amazon Bedrock will generate images based on the descriptions you provide.\n️ Image Actions With Nova Canvas, you can perform actions such as:\nRemove background,\n️ Remove objects,\nGenerate image variations.\nTry running one of these actions on an image you\u0026rsquo;ve created.\nNext Steps Once you\u0026rsquo;re done experimenting, let’s explore how we can bring the power of Amazon Bedrock into real-world applications using the API.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/3-module2/3.2-use-amazon-bedrock-api/3.2.1-generating-images-with-amazon-bedrock/",
	"title": "Using Amazon Bedrock API to Generate Images with Amazon Nova Canvas",
	"tags": [],
	"description": "",
	"content": "In this section, we will explore how to use the Amazon Bedrock API to generate images using Amazon Nova Canvas. We\u0026rsquo;ll complete three coding exercises to build image generation applications with the help of Amazon Q Developer.\nIntroduction to Amazon Nova Amazon Nova is the next generation of advanced foundation models (FMs) that offer cutting-edge intelligence and industry-leading price-performance, exclusively available on Amazon Bedrock.\nYou can use Amazon Nova to reduce cost and latency for nearly any generative AI task. It supports complex document and video analysis, diagram understanding, compelling video content creation, and sophisticated AI agents optimized for enterprise workloads.\nWhether you\u0026rsquo;re developing document-processing applications that handle text and images, generating marketing content at scale, or building AI assistants that can understand and process visual information, Amazon Nova delivers the intelligence and flexibility you need through two model types:\nUnderstanding Models: Accept text, image, or video input and generate text output. Creative Generation Models: Accept text and/or image input and generate image or video output. Amazon Nova Canvas Amazon Nova Canvas is a modern image-generation model that creates professional images from user-provided text or images. It also includes easy-to-use tools for image editing through text instructions, as well as controls to adjust color schemes and layouts.\nThis model includes built-in safety features for responsible AI usage, such as watermarking and content moderation.\nExercise 1: Build a Text-to-Image App We will begin by creating a Streamlit app that generates images from user prompts and predefined styles. Streamlit allows you to easily build interactive web apps in Python.\nGoal Build an app that allows users to:\nSelect a model. Enter a prompt to send to Amazon Bedrock. Display the generated image. The final app will look like this: Gen Image App\nGetting Started In your Visual Studio Code IDE, open the file:\nimage_examples/image_gen_st.py\nThis file contains code for calling different models but is missing the interactive UI components. This is where Amazon Q Developer can help turn our ideas into executable code.\nAmazon Q Developer - Writing Code Since we already know the intended functionality of the app, ask Amazon Q to update the application with the following:\nCan you add the following to my application: 1. A text input box to capture user prompts. 2. A standalone function that can turn a base64 string into an image. 3. A button to call `generate_image_nova` 4. The image return will be a base64 string; use the standalone function to convert it to an image that can be displayed in Streamlit. Amazon Q Developer will process the request and provide updated code for the file.\nAmazon Q Developer - Understanding Code To understand code functionality:\nHighlight the code. Right-click and select: Send to Amazon Q → Explain. Q Developer will return a detailed explanation of the code.\nRun Your Code To test your script:\nRun it from the terminal: streamlit run image_examples/image_gen_st.py Click Open in Browser (or use the external URL).\nIf successful, you will see the application. Now create the image by entering the prompt, for example then click on the Button\na cat jumping into water If errors occur, return to the IDE and use Amazon Q Developer to debug. The complete code is available at: full_code/image_gen_st_full.py\nNext Steps For the next exercise, we will edit images using a prompt.\nYou can save one of the images you\u0026rsquo;ve created or use the image below for practice.\nExercise 2: Build an Image-to-Image App In this exercise, we will build an app that can update an image based on a user prompt.\n️ First, open the file:\nimage_examples/image_to_image_st.py This file already has some code to do image-to-image generation, but it’s missing some interactive components to make it fully functional.\nThis is where we can take advantage of inline code suggestions image_to_image_st.py from Amazon Q Developer to generate the necessary code.\nTasks to Complete In image_to_image_st.py, your task is to add comments at # TODO insert your comments (line 148) that will help generate the following functionality:\nAdd a file uploader to upload an image Get a user prompt to modify the image How to Use Inline Code Suggestions On Mac: press Option + C On Windows: press Alt + C Need help? Try suggestions like those above.\nFull reference code available at: full_code/image_to_image_st_full.py\nRun Your Code When you\u0026rsquo;re ready to test, run this command in your Terminal:\nstreamlit run image_examples/image_to_image_st.py Stop it any time using CTRL + C\nExample prompt for a cat image add a yellow mane to the cat Make sure to click the button you created. Try experimenting with different prompts!\nIn the next exercise, we\u0026rsquo;ll explore more advanced ways to edit images. Exercise 3: Build an Image Understanding App In this part, we’ll explore how to use Claude 3.5 Sonnet, Anthropic’s multimodal AI model, to:\n️ Input an image Receive a detailed text description of the image We’ll also explore how to use the /dev feature from Amazon Q Developer to add new functionality.\nTasks to Complete ️ Open the file:\nimage_examples/image_understanding_st.py This file is missing a few interactive components. You need to:\nDisplay the image in column 1 Add a button to describe the image using Claude Sonnet Use /dev in Amazon Q Developer In the Amazon Q Developer chat window, type:\n/dev Once /dev is bolded, request:\nCan you update image_understanding_st.py to show the image in column 1, and then add a button to describe the image. Amazon Q Developer will:\nCreate a feature plan Show you the proposed changes ️ Let you click Generate code Allow you to review and then click Insert code Full reference code available at:\nfull_code/image_understanding_st_full.py Run Your Code In Terminal, run:\nstreamlit run image_examples/image_understanding_st.py Stop with CTRL + C\nClick the button you created and try with different images!\nWrap-Up Now you know how to use Amazon Bedrock to generate and edit images.\nNext, we’ll explore how to use this tool for text generation tasks!\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/4-module3/4.3-building-agents-for-amazon-bedrock/4.3.2-agents-api/",
	"title": "Agents API",
	"tags": [],
	"description": "",
	"content": "You can also invoke your agent through the API.\nTo try it out:\nHead back to the VSCode Server and open rag_examples/agent_rag.py. Update AGENT_ID with the ID for your Agent. It is in the Agent overview section for the Agent you created. Click the Hamburger Menu in the top left corner. Navigate to Terminal -\u0026gt; New Terminal. Run the code with python3 rag_examples/agent_rag.py. ️Try playing with QUERY on line 6 to see what type of responses you get. "
},
{
	"uri": "http://localhost:1313/genai_workshop/3-module2/3.2-use-amazon-bedrock-api/3.2.2-generating-text-with-amazon-bedrock/",
	"title": "Generating Text with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "In this module, we\u0026rsquo;ll explore how to use Amazon Bedrock Converse API to create conversational applications for:\n️ Summarization Sentiment Analysis Question and Answering (Q\u0026amp;A) The Converse API provides a consistent API that works with all Amazon Bedrock models that support messages. This means you can write code once and use it with different models. Should a model have unique inference parameters, the Converse API also allows you to pass those unique parameters in a model-specific structure.\nWe\u0026rsquo;ll learn how to invoke various foundation models to perform these tasks. Be sure to leverage Amazon Q Developer to help you with the exercises.\n️ Getting Started To start, we will be updating a script to run the text generation examples. Inside your Visual Studio Code IDE, open the file:\ntext_examples/gen_text.py\nHere we have code to invoke the different models, but we are missing logic to make the code functional. This is where we can leverage Amazon Q Developer to turn our ideas into workable code.\nCurrently, the code for text summarization is already filled out. The exercises will guide you to complete the sections for sentiment analysis and Q\u0026amp;A.\nYou can choose any of the models listed on line 58 to help you complete the exercises.\nThe complete reference code is available at:\nfull_code/gen_text.py\nExercise 1: Sentiment Analysis with Amazon Bedrock Sentiment analysis is the tech version of “reading the room.” It gauges the mood or opinion embedded in a piece of text. Businesses use it to understand customer feedback, market trends, and social media conversations—turning qualitative data into actionable insights.\nRight now, the function sentiment_analysis (line 93) is waiting for your input. You must fill out the function with a call to a model and a prompt to perform sentiment analysis.\nYou can view the summarize_text function (line 71) as an example.\nRun the script using this command in the terminal:\npython3 text_examples/gen_text.py The text you’ll be working with is on line 113.\nExercise 2: Q\u0026amp;A with Amazon Bedrock Question and Answer (Q\u0026amp;A) systems are like your own personal research assistant. They help you extract specific answers from a sea of information. This is incredibly useful in scenarios ranging from customer service bots to extracting insights from large datasets.\nYou must fill out the perform_qa function (line 103) with a call to a model and create a prompt that takes in the question and the source text.\nExecute your script with by entering the following code in the Terminal and pressing enter.\npython3 text_examples/gen_text.py Wrap Up Now that you have gotten a taste of using Amazon Bedrock for processing text, let’s explore how we can use it for video tasks next!\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/2-module1/",
	"title": "Module 1",
	"tags": [],
	"description": "",
	"content": "Build Generative AI Applications with PartyRock What is PartyRock? PartyRock is a shareable Generative AI app-building playground that allows you to experiment with prompt engineering in a hands-on and fun way. In just a few clicks, you can build, share, and remix apps to get inspired while playing with Generative AI.\nExample Use Cases: Build an app to generate dad jokes on a topic of your choice. Create and play a virtual trivia game online with friends. Create an AI storyteller to guide your next fantasy roleplaying campaign. By building and playing with PartyRock apps, you’ll learn key techniques for using Generative AI, such as:\nUnderstanding how foundation models respond to prompts ️ Experimenting with prompt engineering Chaining prompts for more complex behaviors Note: You can create a profile using social login from Amazon.com, Apple, or Google.\nPartyRock is separate from the AWS Console and does not require an AWS account.\nExercise 1: Building a PartyRock Application App: MoodWiseBooks Let’s build an app that recommends books based on your mood.\nSteps: Go to the PartyRock website Log in using your Amazon.com, Apple, or Google account Click Generate app Enter the following prompt:\nProvide book recommendations based on your mood and a chat bot to talk about the books Click Generate Using the App PartyRock automatically builds the interface to:\nAccept user input Provide book recommendations Include a chatbot for book discussions Try entering your mood and chat with the bot about the recommended books.\nYou can share your app by clicking the Share button.\nUpdating Your App In PartyRock, each UI element is a Widget. Widgets:\nDisplay content Take user input Connect with other widgets Generate output ️AI-powered Widget Types: ️ Image generation Chatbot Text generation You can edit AI-powered widgets to change their behavior and outputs.\nOther Widget Types: User input – For user interaction Static text – For displaying instructions Document upload – Accepts documents as input For more details, check the PartyRock Guide\nExercise 2: Playtime with PartyRock Try updating the app’s prompts Experiment with chaining outputs from one widget to another Add a widget to draw an image based on the book recommendation Remix an Application Use the Remix feature to:\nMake a copy of any app into your account Customize and extend existing apps Remix public apps from the PartyRock Discover page or your friends Create a Snapshot If you get a funny or interesting response:\nMake sure your app is in public mode Click Snapshot in the top-right corner Share the URL that includes your current input/output Wrap Up PartyRock lets you explore and prototype Generative AI ideas quickly.\nFor production-ready apps, you can take your ideas and implement them using Amazon Bedrock.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/4-module3/4.2-using-amazon-bedrock-knowledge-bases/",
	"title": "Using Amazon Bedrock Knowledge Bases",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock Knowledge Bases provides a fully managed RAG (Retrieval Augmented Generation) service for querying uploaded data. By specifying the location of your data in Amazon S3, the service will:\nAutomatically retrieve documents, Chunk them into text blocks, Convert them to embeddings, Store embeddings in a vector database. There\u0026rsquo;s also an API for building applications that use the Knowledge Base.\nIn this module, we\u0026rsquo;ll create a Knowledge Base using a portion of the AWS Well-Architected Framework documentation.\nExercise 1: Creating a Knowledge Base in the AWS Console Access the Knowledge Base Console. Click the Create button (orange) and select Knowledge Base with vector store. Use the default name or enter a custom name → Click Next. Click Browse S3, select the bucket with a name containing awsdocsbucket → Click Next. Select Titan Embeddings V2 as the embedding model → Keep the default Vector store → Click Next. Scroll down and select Create Knowledge Base. Creating a Knowledge Base takes a few minutes. Don\u0026rsquo;t leave this page.\nWhile waiting, you can complete the survey at: https://pulse.aws/survey/UBCJUUZV\nQuerying a Knowledge Base When your Knowledge Base is ready, you can test it directly in the console interface:\nClick Sync to synchronize the data (takes about 1 minute). Click Select Model, choose Amazon Nova Pro → Click Apply. Enter a question in the Enter your message here box, for example: Can you explain what a VPC is? Click Run to receive a response from the model and view the source documents in the Show result details section. Try different questions to explore further!\nExercise 2: Using the Knowledge Base API You can also query via API with 2 methods:\nretrieve: Returns documents relevant to the question. retrieve_and_generate: Performs the RAG workflow with a model. Implementation: Open the rag_examples/kb_rag.py file in your IDE. Update the KB_ID variable with your Knowledge Base ID (found in the Overview section). Run the command: python3 rag_examples/kb_rag.py Try changing the QUERY variable on line 4 to see different responses. The code is executing the RAG process: converting the question to an embedding, retrieving relevant documents, and answering with the model.\nWrap up After creating a Knowledge Base, the next step will be embedding this Knowledge Base into an Amazon Bedrock Agent to build a more intelligent automated response system.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/4-module3/4.3-building-agents-for-amazon-bedrock/",
	"title": "Building Agents for Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "Amazon Bedrock Agents help you build generative AI assistants that break down user requests into multiple steps. They use developer-provided instructions to create a coordination plan and then execute it by calling APIs and accessing Knowledge Bases to provide a final response to the end user.\nIn this module, we\u0026rsquo;ll create an AWS Solutions Architect Agent that uses our Knowledge Base to answer AWS-related questions. We\u0026rsquo;ll also create an action to read records from a database. Since LLMs are not designed for data analysis, it\u0026rsquo;s better to build a dedicated tool (Lambda) that the agent can call.\nExercise 1: Building an Amazon Bedrock Agent Go to the Agents Console, then click Create Agent.\nFill in the following:\nName: Agent-AWS\nDescription: Agent AWS is an automated, AI-powered agent that helps customers with knowledge of AWS by querying the AWS Well-Architected Framework\nLeave other settings as default and click Create.\nAgent Details Select Amazon Nova Pro as the model.\nProvide the following instruction:\nYou are an expert AWS Certified Solutions Architect. Your role is to help customers understand best practices on building on AWS.\nClick Save at the top.\n️ Action Group An Action is a task the agent can execute automatically by calling a Lambda function. A set of actions is defined in an Action Group, and an OpenAPI schema defines all the APIs in the group. In this exercise, we will use a predefined action that reads records from a database.\nTo create it:\nClick Add in the Action group section. Enter the following information: Action Group Name: data-processing Description: Actions to process data Action group type: Define with API schemas Lambda Function: Select an existing function → data_process_action API Schema: Select an existing OpenAPI schema Click Browse S3, choose the bucket with openapibucket in its name Select the file agent_bedrock_schema.json Click Choose, then click Create Knowledge Base Integration Now we\u0026rsquo;ll add the Knowledge Base we created earlier.\nClick Add in the Knowledge Base section.\nSelect the Knowledge Base created earlier.\nUse this instruction:\nQuery the AWS Well-Architected Framework to answer the customer's question Click Add to finish.\nUsing the Agent To test the Agent in the console:\nClick Prepare in the chat window. Once ready, start asking questions. For example:\nWhat can you tell me about S3 buckets? Once the Agent replies, click Show trace to see the steps it followed to generate the answer.\nYou can also test the record-reading action by asking:\nCan you help with the data processing task of getting the number of records in the production database? ️ It looks like we hit an error. No worries—this was intentional, and we\u0026rsquo;ll fix it in the next section using Amazon Q to debug the function.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/3-module2/",
	"title": "Module 2",
	"tags": [],
	"description": "",
	"content": "Use Foundation Models in Amazon Bedrock Overview Amazon Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) from leading AI companies like Stability AI, Anthropic, and Meta, via a single API. It also provides the capabilities you need to build Generative AI applications with a focus on:\nSecurity Privacy Responsible AI Because Amazon Bedrock is serverless, you don’t need to manage infrastructure. You can securely integrate and deploy Generative AI into your applications using familiar AWS services.\nModule Objective In this module, you\u0026rsquo;ll learn how to use Amazon Bedrock via the:\n️ Console API To generate:\nText ️ Images Model Access Before building with Amazon Bedrock, you need to grant model access to your account.\n️ Steps to Enable Model Access: Go to the Model access page Click the Enable specific models button Select the checkboxes below to activate models (no cost to enable – pay only for usage): Amazon (auto-selects all Amazon models)\nAnthropic → Claude 3.5 Sonnet v2, Claude 3.5 Haiku\nMeta\nMistral AI\nStability AI → SDXL 1.0\nClick Request model access to finalize activation Monitor Model Access Watch the status of each selected model It may take a few minutes for models to move from In Progress ️ Access granted Use the Refresh button to update the status Make sure all selected models show Access granted\nCongratulations! You have successfully configured Amazon Bedrock!\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/3-module2/3.2-use-amazon-bedrock-api/3.2.3-understanding-video-with-amazon-bedrock/",
	"title": "Understanding Video with Amazon Bedrock",
	"tags": [],
	"description": "",
	"content": "In this section, we\u0026rsquo;ll learn how to leverage Amazon Nova for video content analysis and understanding.\nExercise 1: Understanding Video In this exercise, we’ll explore how to use Amazon Nova’s understanding models to analyze video content. Imagine you\u0026rsquo;re building a content management system that automatically generates engaging titles for video content. This could be useful for:\nA media company processing large volumes of videos A marketing team organizing video assets An e-learning platform cataloging educational videos We’ll create a script that analyzes an MP4 file of the sea and uses Nova’s advanced understanding capabilities to generate creative and relevant titles based on the video\u0026rsquo;s content. This demonstrates Nova’s ability to comprehend complex visual information and generate human-like text.\nGetting Started Open the file:\nvideo_examples/video_understanding.py This file already contains code to ingest video and send requests to Amazon Nova. However, it is missing the correct data formatting to send the video to Amazon Bedrock.\nThis is where you can use Amazon Q Developer to make a quick inline edit to complete the missing functionality.\nUsing Amazon Q Developer for Inline Editing To complete the video formatting functionality:\nHighlight the encode_video_to_base64 function on line 17 Press ⌘ + I on Mac or Ctrl + I on Windows Type the following prompt: Finish the TODO and return the correct data Running Your Code Once you\u0026rsquo;re ready to test your script:\ncd video_examples python3 video_understanding.py If successful, you’ll see generated titles for your video.\nIf you encounter errors, use Amazon Q Developer to help debug the issues.\nYou can also reference the complete solution here:\nfull_code/video_understanding_full.py Exploring Nova\u0026rsquo;s Capabilities Once the basic implementation works, try experimenting with different prompts to explore Nova’s power:\nGenerate detailed video summaries for content cataloging Create engaging descriptions for social media Produce scene-by-scene analysis for video editing Extract key moments or highlights for quick previews Generate content tags for improved searchability These use cases show how Nova can be integrated into various content workflows and automation scenarios.\nWrap-Up Now that you\u0026rsquo;ve gained experience using Amazon Bedrock for video understanding and text generation, we’ll move on to one of the most popular use cases in Generative AI:\nChatting with your documents ️\nThis next section will build on what you\u0026rsquo;ve learned and show you how to create interactive experiences with document-based content.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/3-module2/3.2-use-amazon-bedrock-api/",
	"title": "Use Amazon Bedrock API",
	"tags": [],
	"description": "",
	"content": "In this section, we will learn how to use the Amazon Bedrock API to generate text and images. We will also use Amazon Q to help us write code.\nAmazon Q is a generative AI assistant that works where you work—whether in your IDE or the AWS Console. Amazon Q can help write code, troubleshoot issues, and provide relevant resources.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/4-module3/",
	"title": "Module 3",
	"tags": [],
	"description": "",
	"content": "Chat with your Documents The ability to ingest documents and then have an LLM answer questions using relevant context is known as Retrieval Augmented Generation (RAG). This module focuses on building these popular Generative AI solutions, exploring various methods to \u0026ldquo;chat with your documents\u0026rdquo;.\n"
},
{
	"uri": "http://localhost:1313/genai_workshop/5-wrapping-up/",
	"title": "Wrapping Up",
	"tags": [],
	"description": "",
	"content": "And just like that, you\u0026rsquo;ve reached the end of our Generative AI workshop.\nThroughout this journey, you have built GenAI applications and used generative tools to assist with coding and debugging tasks.\nWhat We Covered Started with PartyRock to quickly prototype ideas without writing any code. ️ Built image and text generation apps using Amazon Bedrock. Used Amazon Q Developer to streamline development. Implemented RAG solutions with LangChain, Knowledge Bases, and Agents. We’d Love Your Feedback! Your feedback helps us improve future workshops and tailor them to your needs.\nTake the Survey\nAdditional Resources AWS Generative AI Community PartyRock Guide Amazon Bedrock Code Examples ‍ Getting Started with Amazon Q Workshop Resources Workshop Link ‍ GitHub Repository Now\u0026hellip;.go build! "
},
{
	"uri": "http://localhost:1313/genai_workshop/6-clean-up/",
	"title": "Clean up resources",
	"tags": [],
	"description": "",
	"content": " Note: If you\u0026rsquo;re using AWS-provided accounts, you can skip this section.\nTo avoid incurring unnecessary charges, follow the steps below to delete all the resources you created during this workshop.\nDelete S3 Objects and Buckets Go to the S3 Console. Delete all objects in the following buckets: awsdocsbucket openapiBucket dataanalysisbucket Follow this guide to delete objects. Once emptied, delete the buckets themselves. Delete IAM Roles Open the IAM Console. In the navigation pane, choose Roles. Select the checkboxes next to these roles: AmazonBedrockExecutionRoleForAgents_* AmazonBedrockExecutionRoleForKnowledgeBase_* AWSServiceRoleForAmazonOpenSearchServerless At the top of the page, click Delete. Delete Knowledge Base Open the Knowledge Base Console. Select the Knowledge Base. Click Delete. Type delete to confirm. Delete Agent Open the Agent Console. Select the Agent. Click Delete. Type delete to confirm. Delete Vector Database (OpenSearch Collection) Open the OpenSearch Collections Console. Select the Collection. Click Delete. Type confirm to proceed. Delete the CloudFormation Stack Open the CloudFormation Console. On the Stacks page, select the stack: gen-ai-workshop-cfn. In the details pane, choose Delete. Confirm by selecting Delete stack when prompted. "
},
{
	"uri": "http://localhost:1313/genai_workshop/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/genai_workshop/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]